{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title\n",
    "\n",
    "[Curriculum Learning in Reinforcement Learning Domains: A Framework and Survey](https://doi.org/10.48550/arXiv.2003.04960)\n",
    "\n",
    "## Authors and Year\n",
    "\n",
    "Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E. Taylor, Peter Stone (2020)\n",
    "\n",
    "## Abstract\n",
    "\n",
    "Reinforcement learning (RL) is a popular paradigm for addressing sequential decision tasks in which the agent has only limited environmental feedback. Despite many advances over the past three decades, learning in many domains still requires a large amount of interaction with the environment, which can be prohibitively expensive in realistic scenarios. To address this problem, transfer learning has been applied to reinforcement learning such that experience gained in one task can be leveraged when starting to learn the next, harder task. More recently, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum for the purpose of learning a problem that may otherwise be too difficult to learn from scratch. In this article, we present a framework for curriculum learning (CL) in reinforcement learning, and use it to survey and classify existing CL methods in terms of their assumptions, capabilities, and goals. Finally, we use our framework to find open problems and suggest directions for future RL curriculum learning research.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "향후 최적 후보물질 탐색을 위해 MIN-T의 Docking 관련 QSAR와 RXN2Vec 모두 reward로 사용될 예정임. 서로 다른 두 가지 task의 효용성을 극대화할 수 있는 reward setting을 위한 자료 조사.\n",
    "\n",
    "~~솔직히 few-shot learning도 신약 개발에 적용할 수 있는 지점이 있지 않을까 하는 생각으로 조사해봤던 건데 이건 너무 사기 같아서… fo기…~~\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Why curriculum?\n",
    "\n",
    "- 인간의 학습은 **일련의 curriculum**을 통해 이루어짐(사실은 인간 뿐만 아니라 동물의 학습에도 curricula의 개념이 들어감). 예를 들어, 수학이라는 학문을 배우기 위해서\n",
    "    - 수를 세는 법을 배운다\n",
    "    - 사칙 연산을 배운다\n",
    "    - 구구단을 외운다\n",
    "    - 방정식, 함수를 배운다\n",
    "    - ……\n",
    "        \n",
    "    <p align=\"center\">\n",
    "    <img src=\"Images/Untitled.png\" alt=\"drawing\" width=\"600\"/>\n",
    "        \n",
    "- 이를 잘 활용한 예시가 Quick Chess\n",
    "    \n",
    "    <p align=\"center\">\n",
    "    <img src=\"Images/Untitled 1.png\" alt=\"drawing\" width=\"1000\"/>\n",
    "    \n",
    "    - 아이들에게 5x5 게임으로 체스의 규칙을 익히게 하고, 점점 더 복잡한 rule을 적용하여 최종적으로 체스 게임 전체를 할 수 있게 하는 방식.\n",
    "- Curricula를 이용하여 artificial agent를 학습시키는 아이디어는 1990년대 초반부터 등장. Grammer learning, robotics control problems 등 다양한 분야에서 단순한 작업부터 시작하여 점진적으로 task의 난이도를 높인 결과 **학습 시간이 절약**되고 **task performance가 증가**한다는 결과가 있었음.\n",
    "\n",
    "### Curriculum learning in Reinforcement Learning(RL)\n",
    "\n",
    "- 기본 concept는 **transfer learning**과 유사함.\n",
    "    - 강화 학습에서 **이전에 학습된 knowledge를 다음 학습으로 전달**한다면, **task의 sequence가 curriculum**이 되고, 어려운 task의 **optimal policy에 도달하는 시간을 절약**할 수 있으며 **성능 또한 향상**할 수 있다!!!\n",
    "- 문제점\n",
    "    - 어떻게 curriculum을 만들 것인가???\n",
    "        - 자동화?\n",
    "        - 아니 애초에 curriculum이 뭔데? 어떻게 정의할건데?\n",
    "        - 꼭 순차적으로 curriculum을 짤 필요가 있을까? (Linear할 필요 없음)\n",
    "    - 어떻게 성능 향상을 평가할 것인가???\n",
    "        - 시간에 중점? 성능에 중점?\n",
    "- 아무튼 다양한 분야에 적용되고 있음. 그니까 잘 araboza!\n",
    "\n",
    "## Background\n",
    "\n",
    "### Reinforcement Learning\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"Images/Untitled 2.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "- Discrete한 time step이 진행됨에 따라 agent는 state를 탐색하고, policy $\\pi(a|s)$에 따른 action을 선택하여 다음 state에 도달함.\n",
    "- Agent의 목표는 마지막 timestep $T$까지 계산되는 reward의 누적합 $G_t$를 최대화하는 최적 정책 $\\pi^*$를 찾는 것임.\n",
    "\n",
    "$$\n",
    "G_t = \\sum_{i=1}^{T-t} R_{t+i}\n",
    "$$\n",
    "\n",
    "- 최적 정책 $\\pi^*$를 찾는 방법에는 크게 3가지가 있음.\n",
    "    - Value function approaches\n",
    "        - Policy를 따를 때 각 state에서의 expected return value $v_{\\pi}(s)$를 먼저 학습하고, 이 value function을 최적 정책을 찾는데 사용.\n",
    "        - Model을 알 수 없는 경우 action-value function $q_{\\pi}(s,a)$를 대신 학습하기도 함.\n",
    "        - 최적의 policy를 찾기 위해 간접적으로 value function을 찾고 학습.\n",
    "    - Policy search approaches\n",
    "        - Value function approaches와 다르게 performance measure $J(\\theta)$를 maximize하는 $\\pi_{\\theta}(a|s)$를 직접 찾고 학습.\n",
    "    - Actor-critic methods\n",
    "        - Actor : agent가 action을 선택하는 방식을 결정하는 parameterized policy.\n",
    "        - Critic : policy evaluation method를 통해 actor의 value function을 estimate함.\n",
    "        - Critic이 제시하는 방향에 맞춰 actor의 policy parameter를 update.\n",
    "\n",
    "### Transfer learning\n",
    "\n",
    "- Pre-training → (transfer learning) → fine tuning 다들 아시져?\n",
    "- RL 시작할 때 random policy로 시작하고 task 어려우면 시간 오래 걸리고 그래서 transfer learning 하면 빠르고 효율적이고 뭐 대충 그런 내용이네요\n",
    "- **Transfer learning의 evaluation metrics**\n",
    "    \n",
    "    <p align=\"center\">\n",
    "    <img src=\"Images/Untitled 3.png\" alt=\"drawing\" width=\"1000\"/>\n",
    "    \n",
    "    - Time to threshold : transfer 전후로 model이 얼마나 빨리 expected return을 보이는지 측정\n",
    "    - Asymptotic performance : transfer 전후로 target task에 대한 최종 performance가 얼마나 차이 나는지 측정\n",
    "    - Jumpstart : transfer 전후로 initial performance가 얼마나 차이 나는지 측정\n",
    "    - Total reward ratio : transfer 전후로 고정된 stopping point까지 누적된 total reward 비교\n",
    "\n",
    "## The Curriculum Learning Method\n",
    "\n",
    "### Curricula\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"Images/Untitled 4.png\" alt=\"drawing\" width=\"1000\"/>\n",
    "\n",
    "- Curriculum C = $(\\mathcal{V},\\mathcal{E},g,\\mathcal{T})$$%(\\mathcal{V},E,)$ is a **directed acyclic graph**\n",
    "    - $\\mathcal{V}$ : set of **vertices**\n",
    "    - $\\mathcal{E} \\subseteq \\left\\{(x,y) | (x,y) \\in \\mathcal{V} \\times \\mathcal{V} \\land x \\ne y \\right\\}$ : set of **directed edges**\n",
    "    - $g : \\mathcal{V} \\rightarrow \\mathcal{P}(\\mathcal{D}^\\mathcal{T})$ : function that **associates** vertices to subsets of samples in $\\mathcal{D^\\mathcal{T}}$\n",
    "        - $\\mathcal{P}(\\mathcal{D}^\\mathcal{T})$ : the power set of $\\mathcal{D^\\mathcal{T}}$\n",
    "    - $\\mathcal{T}$ : set of **tasks**\n",
    "    - directed edge $\\left\\langle \\mathcal{v_{\\mathcal{j}},v_{\\mathcal{k}}} \\right\\rangle$ in C : indicates that **samples associated with $\\mathcal{v_{\\mathcal{j}}} \\in \\mathcal{V}$** should be **trained on before** samples associated with $\\mathcal{v_{\\mathcal{k}}} \\in \\mathcal{V}$.\n",
    "        - All paths terminate on a **single sink node** $\\mathcal{v_{\\mathcal{t}}} \\in \\mathcal{V}$.\n",
    "- Curriculum은 online 또는 offline으로 만들어질 수 있음.\n",
    "    - Online → edges are **added dynamically** based on the learning progress of the agent on the samples at a given vertex.\n",
    "    - Offline → graph is **generated before training**, and edges are selected based on properties of the samples associated with different vertices.\n",
    "- Task가 커질수록 curriculum graph를 만드는 것이 어려워지므로, 실전에서는 간소화하여 사용함.\n",
    "    1. Single-task curriculum\n",
    "        \n",
    "        $|\\mathcal{T}|$ = 1이며, target task $m_{t}$ 하나로 구성된 curriculum.\n",
    "        \n",
    "    2. Task-level curriculum\n",
    "        \n",
    "        $g : \\mathcal{V} \\rightarrow \\mathcal{P}\\left\\{\\mathcal{D}_{i}^\\mathcal{T}|m_{i}\\in\\mathcal{T}\\right\\}$. 즉, 각 vertex가 single task $\\mathcal{T}$의 sample들과 연결됨.\n",
    "        \n",
    "    3. Sequence curriculum\n",
    "        \n",
    "        Vertex의 indegree, outdegree가 최대 1이고 1개의 source node와 1개의 sink node를 가진 curriculum.\n",
    "        \n",
    "\n",
    "### Curriculum learning\n",
    "\n",
    "- Curriculum learning은 **agent의 누적된 경험**의 **순서를 최적화**하여 **performance나 training speed를 극대화**하는 방법론.\n",
    "    - Task Generation : Intermediate task의 설정. 미리 설정하거나(pre-specified) curriculum construction 단계에서 agent에 맞춰 유동적으로 생성되게 할 수 있음.\n",
    "    - Sequencing : experience samples $\\mathcal{D}$의 배열 순서 결정. 미리 curricula를 결정할 수도 있으며, 최근에는  자동으로 sequencing을 하는 방식이 연구되고 있음.\n",
    "    - Transfer Learning : 2개 이상의 task에 대한 curriculum을 만들 때 이전 task의 reusable knowledge를 추출하여 다음 task에 전달함.\n",
    "\n",
    "### Evaluating Curricula\n",
    "\n",
    "- 기본적으로 transfer learning과 같은 평가 metric 사용.\n",
    "- Transfer learning과의 차이점 : curriculum 자체를 build하는 과정이 들어간다는 것.\n",
    "    - 사람이 design하는 curriculum → curriculum design에 들어가는 시간이나 노력, 지식 등을 평가하기 어려움.\n",
    "    - 자동화된 design 역시 어느 정도 배경 지식을 기반으로 하기 때문에 평가가 쉽지 않음.\n",
    "    - 결국은 curriculum을 design하는 것이 중요 이슈 중 하나이며, curriculum을 여러개 짜서 서로 성능 비교를 하는 방식으로 평가.\n",
    "\n",
    "### Dimensions of Categorization\n",
    "\n",
    "1. **Intermediate task generation**; target, automatic, domain experts, naive users\n",
    "2. **Curriculum representation**; single, sequence, graph\n",
    "3. **Transfer method**; policies, value function, task model, partial policies, shaping reward, other, no transfer\n",
    "4. **Curriculum sequencer**; automatic, domain experts, naive users\n",
    "5. **Curriculum adaptivity**; static, adaptive\n",
    "6. **Evaluation metric**; time to threshold, asymptotic, jumpstart, total reward\n",
    "7. **Application area**; toy, sim robotics, real robotics, video games, other"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
